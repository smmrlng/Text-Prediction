{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get issue data and split it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_names = [\"ant-ivy\", \"archiva\", \"calcite\", \"cayenne\", \"commons-bcel\", \"commons-beanutils\", \"commons-codec\",\n",
    "            \"commons-collections\", \"commons-compress\", \"commons-configuration\", \"commons-dbcp\", \"commons-digester\",\n",
    "            \"commons-io\", \"commons-jcs\", \"commons-jexl\", \"commons-lang\", \"commons-math\", \"commons-net\",\n",
    "            \"commons-rdf\", \"commons-scxml\", \"commons-validator\", \"commons-vfs\", \"deltaspike\", \"eagle\", \"giraph\", \"gora\",\n",
    "            \"jspwiki\",\"kylin\", \"lens\", \"mahout\", \"manifoldcf\", \"nutch\", \"opennlp\", \"parquet-mr\", \"santuario-java\",\n",
    "            \"systemml\", \"tika\", \"wss4j\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pycoshark.mongomodels import  IssueSystem, Issue\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y_target=[]\n",
    "X_desc=[]\n",
    "\n",
    "for i in np.arange(len(project_names)):\n",
    "    project = Project.objects(name=project_names[i]).only('id').get()\n",
    "    issue_system = IssueSystem.objects(project_id=project.id).only('id','url').get()\n",
    "    for issue in Issue.objects(issue_system_id=issue_system.id).only('desc','issue_type','title').timeout(False):\n",
    "        if(issue.issue_type=='Bug'):\n",
    "            Y_target.append(issue.issue_type)\n",
    "        elif(issue.issue_type==None):\n",
    "            continue\n",
    "        else:\n",
    "            Y_target.append('Non_bug')\n",
    "        if (issue.desc == None):\n",
    "            issue.desc = \"nodescr\"\n",
    "        X_desc.append(issue.title+' '+issue.desc)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_desc, Y_target, test_size=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "def cleandata(X_train):\n",
    "    for i in np.arange(len(X_train)):\n",
    "        if(X_train[i]==None):\n",
    "            print(X_train[i])\n",
    "            X_train[i]=\"nodescription\" \n",
    "            continue    \n",
    "        X_train[i] = re.sub(\"http(.)*\",\" link\",X_train[i])\n",
    "        X_train[i] = re.sub(\"(\\r|\\n|\\r\\n)\",\" \",X_train[i])\n",
    "        X_train[i] = re.sub(\"{code(.)*}*[.\\n]*{code(.)*}\",\"code\",X_train[i])\n",
    "        X_train[i] = re.sub(\"{noforamt(.)*}(.)*{noformat(.)*}\",\"code\",X_train[i])\n",
    "        X_train[i] = re.sub(\"[a-z]*\\{(.)*\\}\",\"code\",X_train[i])\n",
    "        X_train[i] = re.sub(\"({) .*(})\",\"code\",X_train[i])\n",
    "        X_train[i] = re.sub(\"[^\\s]*\\d[^\\s]*\",\" \",X_train[i])\n",
    "        X_train[i] = re.sub(\"[^\\s]{15,}?\", \"\", X_train[i])\n",
    "        X_train[i] = X_train[i].lower()\n",
    "        exclude = set(string.punctuation)\n",
    "        X_train[i] = ''.join(ch for ch in X_train[i] if ch not in exclude)\n",
    "    return X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id like propos ad zipf distribut commonsmath patch incomplet somewhat ineffici id like throw discuss though follow']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = [\"I'd like propose adding Zipf distribution to commons-math. I have a patch, but it's incomplete and somewhat inefficient; I'd like to throw it up for discussion though. To follow.\"]\n",
    "cleandata(sample)\n",
    "preparedata(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem and Tokenize and Shape the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "def preparedata(X_train):\n",
    "    for i in np.arange(len(X_train)):\n",
    "        if(X_train[i]==None):\n",
    "            print(X_train[i])\n",
    "            X_train[i]=\"nodescription\" \n",
    "        X_train[i] = word_tokenize(X_train[i])\n",
    "        stopWords = set(stopwords.words('english'))\n",
    "        filtered = [w for w in X_train[i] if not w in stopWords]\n",
    "        X_train[i] = [PorterStemmer().stem(w) for w in filtered]\n",
    "        X_train[i] = ' '.join(X_train[i])\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "def shapedata(X_train):\n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(X_train)\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "    return X_train_tfidf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Testbug= ['work','Fine thanks', 'bad', 'good', 'code']\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X_train =cleandata(X_train)\n",
    "\n",
    "X_train=preparedata(X_train)\n",
    "#print(X_train)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "clf = MultinomialNB().fit(X_train_tfidf, Y_train)\n",
    "\n",
    "Test = cleandata(X_test)\n",
    "Test = preparedata(Test)\n",
    "X_new = count_vect.transform(Test)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7768644331500881\n",
      "0.11300694948656778\n",
      "0.11012861736334405\n"
     ]
    }
   ],
   "source": [
    "#correct predictions\n",
    "print(np.mean(predicted==Y_test))\n",
    "#wrong as bug predicted\n",
    "print(np.mean((predicted=='Bug')&(predicted!=Y_test)))\n",
    "#wrong as non bug predicted\n",
    "print(np.mean((predicted!='Bug')&(predicted!=Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=1000, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "n = 1000 # maximal depth\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=n, random_state=0)\n",
    "clf.fit(X_train_tfidf,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.739679493828441\n",
      "0.15351104657193237\n",
      "0.1068094595996266\n"
     ]
    }
   ],
   "source": [
    "#correct predictions\n",
    "print(np.mean(predicted==Y_test))\n",
    "#wrong as bug predicted\n",
    "print(np.mean((predicted=='Bug')&(predicted!=Y_test)))\n",
    "#wrong as non bug predicted\n",
    "print(np.mean((predicted!='Bug')&(predicted!=Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
